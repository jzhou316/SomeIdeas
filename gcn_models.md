- Extend the Markov process in GCN to **continuous time Markov chain**, with something similar to [Neural ODE](https://papers.nips.cc/paper/7892-neural-ordinary-differential-equations.pdf).\
  Aggregate information continuously (with no discrete steps) using integrals or ODEs. The benefits might be memory efficiency?\
  Reference (continuous time Markov chain): http://www.randomservices.org/random/markov/Continuous.html
  
- Some insights from [power law of degree distribution](http://www.math.pitt.edu/~lewicka/Semester_DiscrNetw_14/MNlecture22.pdf).\
  For example, in Epidemiology, infection model (with a set of differential equations as seen in AM50 I TFed).
  
- Distributional learning on a graph using GCN, i.e. in privacy settings such as the one described in the BotGrep paper.
  Guannan's methods for distributed optimization.
